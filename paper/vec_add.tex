\section{Modern C++ by example}\label{modern}

This section introduces some features of modern C++ by example. It focuses on the features most important in the context of HPC and performancy analysis, i.e. concurrency, synchronisation and memory management. The first example (Section \ref{modern:vec_add}) shows the implementation of a simple CUDA kernel and its surroundings in modern C++, the second example (Section \ref{modern:queue}) a thread-safe producer/consumer queue.

\subsection{CUDA Vector Addition}\label{modern:vec_add}

Using the CUDA toolkit, this subsection introduces some of the features supported by the C++11 and C++14 standards. It focuses on additions to the core language which improve maintainability and the stating of the programmer's intent as well as dynamic memory management.

\subsubsection{The Kernel}\label{modern:vec_add:kernel}

Since CUDA 7 a subset of C++11 is supported. A kernel which implements a vector addition looks like the following code:

\begin{minted}[breaklines,breakafter=\,,fontsize=\small]{cuda}
__global__ void vector_add(const std::int32_t* a, const std::int32_t* b, std::int32_t* c, std::size_t size)
{
    auto i = threadIdx.x + blockIdx.x * blockDim.x;
    if(i < size) c[i] = a[i] + b[i];
}
\end{minted}

\noindent There are two notable differences to traditional code: firstly the usage of \texttt{std::int32\_t} instead of plain \texttt{int} and secondly automatic type deduction with the keyword \texttt{auto}. Both are presented in the following paragraphs.

\paragraph{Fixed-Width Integer Types}

Instead of \texttt{int} the kernel parameters are of the type \texttt{std::int32\_t}. The latter is called a \textit{fixed-width integer type} and was introduced with the C++11 standard~\cite{cpp11std}(§18.4.1) which in turn based this inclusion on the C99 standard~\cite{c99std}(§7.18). These types (ranging from 8-bit to 64-bit types, both signed and unsigned), along with counterparts optimized for size or performance, can be found in the header file \texttt{<cstdint>}.

\paragraph{The \texttt{auto} Keyword}

In C++11 the already existing keyword \texttt{auto} changed its meaning. Prior to C++11 it denoted the storage class of a variable, a property inherited from C~\cite{c99std}(§6.7.1):

\begin{minted}[fontsize=\small]{c++}
void f()
{
    auto int i; // local variable, lives until end of scope
    static int j; // static variable, permanent duration
}
\end{minted}

\noindent Because the usage of this keyword was largely redundant, its meaning was changed in C++11~\cite{cpp11std}(§7.1.6.4) to deduce the type of an expression:

\begin{minted}[fontsize=\small]{c++}
auto i = 0; // i is deduced to be an int
auto f1 = float(0); // committing to a specific type
auto f2 = 0.f; // committing to a specific type, shorthand
auto two = std::sqrt(4); // type deduction from return value
\end{minted}

\noindent\texttt{auto} can also be used in combination with functions:

\begin{minted}[fontsize=\small]{c++}
auto f() -> void;

template <class T, class U>
auto add(T a, U b) -> decltype(a + b)
{
    return a + b;
}
\end{minted}

\noindent The \textit{trailing return type} syntax is especially needed when used with templates, as the second example shows. \texttt{decltype} is the counterpart to \texttt{auto} which returns the type of an expression (must be evaluable at compile-time).

Since C++14 the return type is no longer needed as long as the function body is visible to the compiler and the function itself is not recursive:

\begin{minted}[fontsize=\small]{c++}
template <class T, class U>
auto add(T a, U b)
{
    return a + b;
}
\end{minted}

\noindent According to Herb Sutter~\cite{sutteraaa} one should follow an \textit{Almost Always Auto (AAA)} pattern, i.e. use \texttt{auto} wherever possible. It can also be shown that the adoption of this practice leads to better performing and more secure code~\cite{sutter_cppcon}.

\subsubsection{Host Memory Management}\label{modern:vec_add:host_mem}

\paragraph{Vector Creation}

Before the kernel can be executed the input vectors need to be initialized, which is typically done on the host. Before the adoption of C++11 the typical approach would have been
\begin{itemize}
\item the definition of a size constant using the preprocessor,
\item memory allocation with \texttt{new} or \texttt{malloc},
\item memory initialization with a \texttt{for}-loop,
\item after kernel execution, freeing memory with \texttt{delete} or \texttt{free}.
\end{itemize}

\noindent However, this is not idiomatic and contradicts the design philosophy mentioned in Section \ref{intro:philosophy}. A more modern approach looks like the following code:

\begin{minted}[fontsize=\small]{c++}
constexpr auto size = 1000;

auto host_a = std::make_unique<std::int32_t[]>(size);
auto host_b = std::make_unique<std::int32_t[]>(size);

std::generate(a.get(), a.get() + size, std::rand);
std::generate(b.get(), b.get() + size, std::rand);

// ...
\end{minted}

\noindent This approach offers several advantages: the need for the preprocessor is eliminated (see the \texttt{constexpr} paragraph in this section), the memory is automatically \texttt{free}'d once the pointers leave their scope (see the ``Smart Pointers'' paragraph in this section) and the usage of \texttt{std::generate} expresses intent much better than a C-style \texttt{for}-loop.

\paragraph{The \texttt{constexpr} Keyword}

\texttt{constexpr} is a new keyword introduced in C++11~\cite{cpp11std}(§20.7). Its purpose is to provide a hint to the compiler that an expression or function can be evaluated at compile-time:

\begin{minted}[fontsize=\small]{c++}
constexpr auto i = 0;

constexpr auto max(std::int32_t a, std::int32_t b)
{
    return (a > b) ? a : b;
}

constexpr auto i = max(2, 3);
auto j = max(x, y);
\end{minted}

\noindent Note that the last expression might be executed at runtime, unless \texttt{x} and \texttt{y} are known at compile-time. However, as \texttt{constexpr} functions are usually small the compiler is likely to inline the function or optimize the call out anyway.

The benefits are not obvious when comparing \texttt{constexpr} constants to preprocessor constants but they become clearer when looking at \texttt{constexpr} functions. Like a lot of other novelties introduced with C++11 \texttt{constexpr}'s primary purpose is to state \textit{intent}. Assume that the following function was provided by developer A:

\begin{minted}[fontsize=\small]{c++}
auto sum(std::uint32_t n) -> std::uint32_t
{
    return n > 0 ? n + sum(n - 1) : n;
}
\end{minted}

\noindent If \texttt{n} is known at compile-time the compiler can easily prove that the function as a whole can be evaluated at compile-time as well, leading to the following:

\begin{minted}[fontsize=\small]{c++}
float arr[sum(10)]; // works
\end{minted}

\noindent Somewhat later developer B decides to change the function in order to make it print the result:

\begin{minted}[fontsize=\small]{c++}
auto sum(std::uint32_t n) -> std::uint32_t
{
    auto res = n > 0 ? n + sum(n - 1) : n;
    std::cout << res << std::endl;
    return res;
}
\end{minted}

\noindent Now every context in which \texttt{sum} was evaluated at compile-time is broken\footnote{GCC 6.2.1 and clang 3.8.1 accept this code. Note that this behaviour is not conforming to the C++ standard. When activating the \texttt{pedantic} flag both compilers will warn about the usage of this extension.}. However, this is the fault of developer A as he never promised that the function was usable in such a context. With \texttt{constexpr} he could enforce his intent and prevent other developers from breaking the code base because the compilation would fail immediately.

\paragraph{Smart Pointers}

Smart pointers are an addition to the standard library introduced in C++11~\cite{cpp11std}(§20.7). Currently there are three variants:
\begin{itemize}
\item \texttt{unique\_ptr}
\item \texttt{shared\_ptr}
\item \texttt{weak\_ptr}
\end{itemize}

At the same time the predecessor \texttt{auto\_ptr} was marked as deprecated~\cite{cpp11std}(§D.10).
\begin{itemize}
\item \texttt{unique\_ptr} is used when there is only one owner of the pointer, for example in a function-local context, as a class member variable or as nodes of a tree.

\item \texttt{shared\_ptr} is the counterpart for situations in which shared ownership is needed, for example nodes in a graph were each node can have multiple parents.

\item \texttt{weak\_ptr} is a ``pointer to pointer''-like construct as it has to be converted to a \texttt{shared\_ptr} before being able to access the memory.
\end{itemize}

Memory is allocated in the following way:
\begin{minted}[breaklines,breakafter=\{,fontsize=\small]{c++}
auto unique_obj = std::unique_ptr<my_class>{new my_class(args)};
auto unique_arr = std::unique_ptr<std::int32_t[]>{new std::int32_t[size]};
// make_unique was introduced in C++14
auto unique_obj = std::make_unique<my_class>(args);
auto unique_arr = std::make_unique<std::int32_t[]>(size);

auto shared_obj = std::make_shared<my_class>(args);
auto shared_arr = std::make_shared<std::int32_t[]>(size);

auto weak_obj = std::weak_ptr<my_class>(shared_obj);
auto weak_arr = std::weak_ptr<std::int32_t[]>(shared_arr);
\end{minted}

\noindent The big benefit of smart pointers becomes clear when looking at how memory is \texttt{free}'d: This is done automatically once the pointer goes out of scope (\texttt{unique\_ptr}) or the internal reference counter is set to zero (\texttt{shared\_ptr}). This essentially makes \texttt{new} and \texttt{delete} obsolete with no (\texttt{unique\_ptr}) to low (\texttt{shared\_ptr} because of reference counting) overhead.

\subsubsection{Device Memory Management}\label{modern:vec_add:dev_mem}

After the host memory has been initialized the vectors have to be copied to device memory in order to be processed by the kernel. Traditionally one would

\begin{itemize}
\item allocate device memory using \texttt{cudaMalloc},
\item copy the memory from host to device and back to the host after kernel execution using \texttt{cudaMemcpy} and
\item finally free the memory using \texttt{cudaFree}.
\end{itemize}

As this is plain C style it does not correspond to the modern design philosophy mentioned in Section \ref{intro:philosophy}. The whole process feels a lot more idiomatic if the calls to \texttt{cudaMalloc} and \texttt{cudaFree} are wrapped in order to use them in a fashion similar to the host memory functions described in Section \ref{modern:vec_add:host_mem}:

\begin{minted}[fontsize=\small]{c++}
auto dev_a = make_device_ptr<std::int32_t>(size);
auto dev_b = make_device_ptr<std::int32_t>(size);
\end{minted}

\noindent This approach also removes the explicit call to \texttt{cudaFree} which would be needed in multiple locations otherwise in order to catch CUDA runtime errors and behave accordingly. The \texttt{make\_device\_ptr} function in this example is implemented as follows:

\begin{minted}[fontsize=\small]{c++}
struct device_deleter { 
    auto operator()(void* p) -> void { cudaFree(p); }
};

template <class T>
using device_ptr = std::unique_ptr<T[], device_deleter>;

template <class T>
auto make_device_ptr(std::size_t size) -> device_ptr<T>
{
    auto p = static_cast<T*>(nullptr);
    cudaMalloc(reinterpret_cast<void**>(&p), size * sizeof(T));
    return device_ptr<T>(p);
}
\end{minted}

\noindent Note the \texttt{using} directive above which is a new way of declaring \texttt{typedef}s. This keyword is presented later in this section.

Once the device memory is allocated, it can be copied from the host to the device. Because the smart pointer itself cannot be passed to \texttt{cudaMemcpy}, we need to access the ``raw'' pointer which can be done by using the \texttt{get} member function:

\begin{minted}[fontsize=\small]{c++}
auto bytes = size * sizeof(std::int32_t);
cudaMemcpy(dev_a.get(), host_a.get(), bytes, cudaMemcpyHostToDevice);
cudaMemcpy(dev_b.get(), host_b.get(), bytes, cudaMemcpyHostToDevice);
// ...
cudaMemcpy(host_c.get(), dev_c.get(), bytes, cudaMemcpyDeviceToHost);
\end{minted}

\noindent For the sake of clarity, error handling is omitted. However, in a real-world application one would need to catch all CUDA runtime errors which are returned by calls to all CUDA functions. If an error occurs the allocated memory (both host and device) needs to be \texttt{free}'d before returning, or else memory is leaked. This adds a lot of ``boilerplate'' code which can be avoided by relying on smart pointers.

\paragraph{The \texttt{using} Keyword}

The \texttt{using} keyword was adapted to serve as another form of \texttt{typedef} in C++11~\cite{cpp11std}(§7.1.3/2). It provides some benefits over classic \texttt{typedef} as the following examples show:

\begin{minted}[fontsize=\small]{c++}
typedef int my_type;
using my_type = int;

typedef void (*func_ptr)(double);
using func_ptr = void (*)(double);

// not possible with typedef
template <class T>
using my_vec = std::vector<T, my_allocator>;
\end{minted}

\noindent While the first example does not change much, the second example shows that function pointer definitions can be expressed a lot clearer because the type name is now clearly separated from the definition. Additionally, the ability to define templated \texttt{typedef}s is a big advantage of its own as this was not possible before and adds a lot more flexibility to the language.

This is also coherent with the new \texttt{auto} style of variable definitions, i.e. the separation of the variable or type name and the corresponding definition. This is becoming a pattern of itself and is increasingly advertised by C++ experts~\cite{sutter_cppcon}.

\begin{minted}[fontsize=\small]{c++}
auto var = 0;
using type = int;
\end{minted}

\subsubsection{Launching the Kernel}\label{modern:vec_add:kernel_launch}

Once all the preparations are done the kernel can be executed. Usually one would call the kernel, specify the grid and block dimensions and pass the parameters. Because there are small kernels for which a manual configuration of the dimensions are useless (as it would not provide a performance benefit), it would be much better to have a simple \texttt{cuda\_launch} function. This function would use mechanisms provided by the CUDA runtime to calculate these dimensions. In this example \texttt{cuda\_launch} is implemented in the following way:

\begin{minted}[fontsize=\small]{c++}
template <class... Args>
auto cuda_launch(void (*kernel)(Args...), Args... args) -> void
{
    // omitted: calculate grid_size and block_size
    kernel<<<grid_size, block_size>>>(args...);
}
\end{minted}

\noindent By making use of \textit{variadic templates} we can pass any kernel with an arbitrary number of arguments to \texttt{cuda\_launch} which will determine the dimension configuration and then execute the kernel accordingly.

\paragraph{Variadic Templates}

Variadic templates are a language extensions introduced in C++11~\cite{cpp11std}(§14.2.15). They are somewhat similar to variadic functions known from C (see \texttt{printf} for an example), except that their \textit{parameter packs} are unpacked at compile-time which leads to different function signatures for different parameters. Additionally, they are usable outside of functions as well, for example in template meta programming.

\subsubsection{Summary}\label{modern:vec_add:summary}

By combining the modernizations mentioned in the previous sections we get the following program:

\begin{minted}[fontsize=\small]{c++}
constexpr auto size = 1000;

auto host_a = make_unique<std::int32_t[]>(size);
auto host_b = make_unique<std::int32_t[]>(size);
auto host_c = make_unique<std::int32_t[]>(size);

auto dev_a = make_device_ptr<std::int32_t>(size);
auto dev_b = make_device_ptr<std::int32_t>(size);
auto dev_c = make_device_ptr<std::int32_t>(size);

std::generate(host_a.get(), host_a.get() + size, std::rand);
std::generate(host_b.get(), host_b.get() + size, std::rand);

auto bytes = size * sizeof(std::int32_t);
cudaMemcpy(dev_a.get(), host_a.get(), bytes, cudaMemcpyHostToDevice);
cudaMemcpy(dev_b.get(), host_b.get(), bytes, cudaMemcpyHostToDevice);

cuda_launch(vec_add, dev_a.get(), dev_b.get(), dev_c.get(), size);

cudaMemcpy(host_c.get(), dev_c.get(), bytes, cudaMemcpyDeviceToHost);
\end{minted}

\noindent This is more idiomatic and cleaner than its C equivalent while providing the benefits of \gls{raii} with regard to host and CUDA device memory. By providing own versions of \texttt{unique\_ptr} with additional metadata it would be easy to wrap the calls to \texttt{cudaMemcpy} and remove the need for size calculations and \texttt{enum} values for the copy direction.